# Optimized TabTransformer v2.0

## ğŸ“‹ ä¼˜åŒ–æ€»ç»“

| ä¼˜åŒ–é¡¹ | åŸç‰ˆ | ä¼˜åŒ–ç‰ˆ | é¢„æœŸæ”¶ç›Š |
|--------|------|--------|----------|
| **æ¶æ„** |
| LayerNorm ä½ç½® | Post-LN | Pre-LN | æ›´ç¨³å®šè®­ç»ƒï¼Œé¿å…æ¢¯åº¦çˆ†ç‚¸ |
| æ¿€æ´»å‡½æ•° | ReLU | GELU | æ›´å¹³æ»‘æ¢¯åº¦ï¼ŒåŠ é€Ÿæ”¶æ•› |
| æ³¨æ„åŠ›æœºåˆ¶ | åˆ†ç¦» QKV | åˆå¹¶ QKV | å‡å°‘å†…å­˜å ç”¨ 10-15% |
| åˆ—åµŒå…¥ | âŒ | âœ… | åŒºåˆ†ä¸åŒç‰¹å¾åˆ— |
| æ•°å€¼ç‰¹å¾å¤„ç† | LayerNorm | Tokenization | æ›´å¥½çš„ç‰¹å¾äº¤äº’ (FT-Transformer) |
| ç‰¹å¾èšåˆ | Flatten | CLS Token | å‚æ•°æ›´å°‘ï¼Œæ›´é«˜æ•ˆ |
| MLP | æ™®é€šçº¿æ€§å±‚ | æ®‹å·®è¿æ¥ | æ›´æ·±ç½‘ç»œæ›´ç¨³å®š |
| **è®­ç»ƒ** |
| æŸå¤±å‡½æ•° | MSELoss | SmoothL1Loss | å¯¹å¼‚å¸¸å€¼æ›´é²æ£’ |
| ä¼˜åŒ–å™¨ | AdamW | AdamW + Lookahead | æ›´å¥½æ³›åŒ– |
| å­¦ä¹ ç‡è°ƒåº¦ | OneCycleLR | CosineAnnealingWarmRestarts | å¤šæ¬¡é‡å¯ï¼Œè·³å‡ºå±€éƒ¨æœ€ä¼˜ |
| SWA | âŒ | âœ… | æ›´å¥½çš„æ³›åŒ–æ€§èƒ½ |
| æ•°æ®å¢å¼º | âŒ | MixUp | é˜²æ­¢è¿‡æ‹Ÿåˆ |
| æ¢¯åº¦è£å‰ª | âœ… | âœ… | é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ |
| PyTorch 2.0 ç¼–è¯‘ | âŒ | âœ… | åŠ é€Ÿ 10-30% |

## ğŸš€ å¿«é€Ÿå¼€å§‹

### åŸºæœ¬ä½¿ç”¨

```python
from optimized_tab_transformer import (
    OptimizedTabTransformerTrainer,
    ModelConfig,
    TrainingConfig,
    LossType
)

# 1. é…ç½®æ¨¡å‹
model_config = ModelConfig(
    embedding_dim=32,
    num_heads=4,
    num_transformer_layers=3,
    d_ff=128,
    dropout=0.1,
    use_numerical_tokenization=True,  # FT-Transformer é£æ ¼
    use_cls_token=True
)

# 2. é…ç½®è®­ç»ƒ
training_config = TrainingConfig(
    batch_size=512,
    epochs=300,
    learning_rate=1e-3,
    use_mixup=True,
    use_lookahead=True,
    use_swa=True
)

# 3. åˆå§‹åŒ–è®­ç»ƒå™¨
trainer = OptimizedTabTransformerTrainer(
    model_config=model_config,
    training_config=training_config
)

# 4. å‡†å¤‡æ•°æ®
train_data, test_data = trainer.prepare_data('path/to/data.csv')

# 5. åˆ›å»ºæ•°æ®åŠ è½½å™¨
train_loader, test_loader = trainer.create_data_loaders(train_data, test_data)

# 6. æ„å»ºæ¨¡å‹
trainer.build_model()

# 7. è®­ç»ƒ
history = trainer.train(train_loader, test_loader, loss_type=LossType.SMOOTH_L1)

# 8. è¯„ä¼°
actuals, preds, metrics = trainer.evaluate(test_loader)
```

## ğŸ”§ è¯¦ç»†é…ç½®è¯´æ˜

### ModelConfig å‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `embedding_dim` | int | 32 | åµŒå…¥ç»´åº¦ |
| `num_heads` | int | 4 | æ³¨æ„åŠ›å¤´æ•° (å»ºè®®: embedding_dim / num_heads >= 8) |
| `num_transformer_layers` | int | 3 | Transformer å±‚æ•° |
| `d_ff` | int | 128 | FFN éšè—å±‚ç»´åº¦ |
| `dropout` | float | 0.1 | Dropout æ¯”ä¾‹ |
| `hidden_dims` | List[int] | [128, 64] | MLP éšè—å±‚ç»´åº¦ |
| `use_numerical_tokenization` | bool | True | æ˜¯å¦ä½¿ç”¨æ•°å€¼ç‰¹å¾ tokenization |
| `use_cls_token` | bool | True | æ˜¯å¦ä½¿ç”¨ CLS token èšåˆ |

### TrainingConfig å‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `batch_size` | int | 512 | æ‰¹æ¬¡å¤§å° |
| `epochs` | int | 300 | æœ€å¤§è®­ç»ƒè½®æ•° |
| `learning_rate` | float | 1e-3 | åˆå§‹å­¦ä¹ ç‡ |
| `weight_decay` | float | 1e-4 | æƒé‡è¡°å‡ |
| `patience` | int | 20 | æ—©åœè€å¿ƒå€¼ |
| `use_mixup` | bool | True | æ˜¯å¦ä½¿ç”¨ MixUp æ•°æ®å¢å¼º |
| `mixup_alpha` | float | 0.2 | MixUp alpha å‚æ•° |
| `use_lookahead` | bool | True | æ˜¯å¦ä½¿ç”¨ Lookahead ä¼˜åŒ–å™¨ |
| `lookahead_k` | int | 5 | Lookahead æ›´æ–°æ­¥æ•° |
| `lookahead_alpha` | float | 0.5 | Lookahead æ’å€¼ç³»æ•° |
| `use_swa` | bool | True | æ˜¯å¦ä½¿ç”¨ SWA |
| `swa_start_epoch` | int | 50 | SWA å¼€å§‹è½®æ•° |
| `swa_lr` | float | 5e-4 | SWA å­¦ä¹ ç‡ |
| `use_weighted_sampling` | bool | True | æ˜¯å¦ä½¿ç”¨åŠ æƒé‡‡æ · |
| `sampling_alpha` | float | 0.5 | é‡‡æ ·æƒé‡å¹³æ»‘ç³»æ•° |
| `gradient_clip` | float | 1.0 | æ¢¯åº¦è£å‰ªé˜ˆå€¼ |
| `use_compile` | bool | True | æ˜¯å¦ä½¿ç”¨ PyTorch 2.0 ç¼–è¯‘ |

## ğŸ“Š æŸå¤±å‡½æ•°é€‰æ‹©

```python
from optimized_tab_transformer import LossType

# å¯ç”¨çš„æŸå¤±å‡½æ•°
LossType.MSE         # å‡æ–¹è¯¯å·® (å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ)
LossType.SMOOTH_L1   # å¹³æ»‘ L1 æŸå¤± (æ¨èï¼Œå¯¹å¼‚å¸¸å€¼é²æ£’)
LossType.HUBER       # Huber æŸå¤±
LossType.COMBINED    # MSE + L1 ç»„åˆæŸå¤±
```

## ğŸ¯ è¶…å‚æ•°è°ƒä¼˜å»ºè®®

### å°æ•°æ®é›† (< 10K æ ·æœ¬)
```python
model_config = ModelConfig(
    embedding_dim=16,
    num_heads=2,
    num_transformer_layers=2,
    d_ff=64,
    dropout=0.2,  # å¢åŠ  dropout
    hidden_dims=[64, 32]
)

training_config = TrainingConfig(
    batch_size=128,
    use_mixup=True,
    mixup_alpha=0.3,  # å¢åŠ  mixup å¼ºåº¦
    patience=30  # æ›´é•¿çš„è€å¿ƒ
)
```

### ä¸­ç­‰æ•°æ®é›† (10K - 100K æ ·æœ¬)
```python
model_config = ModelConfig(
    embedding_dim=32,
    num_heads=4,
    num_transformer_layers=3,
    d_ff=128,
    dropout=0.1,
    hidden_dims=[128, 64]
)

training_config = TrainingConfig(
    batch_size=512,
    use_mixup=True,
    use_swa=True
)
```

### å¤§æ•°æ®é›† (> 100K æ ·æœ¬)
```python
model_config = ModelConfig(
    embedding_dim=64,
    num_heads=8,
    num_transformer_layers=4,
    d_ff=256,
    dropout=0.1,
    hidden_dims=[256, 128, 64]
)

training_config = TrainingConfig(
    batch_size=1024,
    use_mixup=False,  # æ•°æ®è¶³å¤Ÿï¼Œå¯ä»¥ä¸ç”¨
    use_swa=True
)
```

## ğŸ” å…³é”®ä¼˜åŒ–è¯¦è§£

### 1. Pre-LayerNorm vs Post-LayerNorm

```
Post-LN (åŸç‰ˆ):
x -> Attention -> Add -> LayerNorm -> FFN -> Add -> LayerNorm

Pre-LN (ä¼˜åŒ–ç‰ˆ):
x -> LayerNorm -> Attention -> Add -> LayerNorm -> FFN -> Add

ä¼˜ç‚¹: æ¢¯åº¦æµæ›´ç¨³å®šï¼Œæ”¯æŒæ›´æ·±çš„ç½‘ç»œ
```

### 2. æ•°å€¼ç‰¹å¾ Tokenization (FT-Transformer)

```python
# åŸç‰ˆ: ä»… LayerNorm
numerical_normalized = self.layer_norm(numerical_inputs)

# ä¼˜åŒ–ç‰ˆ: æ¯ä¸ªæ•°å€¼ç‰¹å¾å˜æˆä¸€ä¸ª token
# [batch, num_features] -> [batch, num_features, embedding_dim]
num_tokens = self.numerical_tokenizer(numerical_inputs)
```

### 3. Lookahead ä¼˜åŒ–å™¨

```
Fast weights: æ­£å¸¸çš„ Adam æ›´æ–°
Slow weights: æ¯ k æ­¥ï¼Œç”¨ fast weights æ›´æ–°ä¸€æ¬¡

slow = slow + alpha * (fast - slow)

æ•ˆæœ: æ›´å¹³æ»‘çš„ä¼˜åŒ–è½¨è¿¹ï¼Œæ›´å¥½çš„æ³›åŒ–
```

### 4. MixUp æ•°æ®å¢å¼º

```python
# æ•°å€¼ç‰¹å¾: çº¿æ€§æ’å€¼
mixed_num = lambda * num_a + (1 - lambda) * num_b

# ç±»åˆ«ç‰¹å¾: éšæœºé€‰æ‹©
mixed_cat = random_choice(cat_a, cat_b)

# ç›®æ ‡: çº¿æ€§æ’å€¼
mixed_y = lambda * y_a + (1 - lambda) * y_b
```

### 5. SWA (Stochastic Weight Averaging)

```
è®­ç»ƒåæœŸï¼Œæ¯ä¸ª epoch ç»“æŸå:
swa_model.update_parameters(model)

æ•ˆæœ: æ›´å¹³å¦çš„æŸå¤±é¢ï¼Œæ›´å¥½çš„æ³›åŒ–
```

## ğŸ“ˆ æ€§èƒ½å¯¹æ¯” (é¢„æœŸ)

| æŒ‡æ ‡ | åŸç‰ˆ | ä¼˜åŒ–ç‰ˆ | æå‡ |
|------|------|--------|------|
| æ”¶æ•›é€Ÿåº¦ | 100% | 70-80% | 20-30% æ›´å¿« |
| æœ€ç»ˆ RÂ² | baseline | +2-5% | æ›´å¥½æ‹Ÿåˆ |
| æ³›åŒ–å·®è· | baseline | -30-50% | æ›´å°è¿‡æ‹Ÿåˆ |
| æ¨ç†é€Ÿåº¦ | 100% | 80-90%* | 10-20% æ›´å¿« |

*ä½¿ç”¨ PyTorch 2.0 ç¼–è¯‘

## ğŸ› å¸¸è§é—®é¢˜

### Q: è®­ç»ƒä¸ç¨³å®š / Loss éœ‡è¡
```python
# é™ä½å­¦ä¹ ç‡
training_config = TrainingConfig(learning_rate=5e-4)

# å¢åŠ æ¢¯åº¦è£å‰ª
training_config = TrainingConfig(gradient_clip=0.5)

# ä½¿ç”¨æ›´å°çš„ batch size
training_config = TrainingConfig(batch_size=256)
```

### Q: è¿‡æ‹Ÿåˆ
```python
# å¢åŠ  dropout
model_config = ModelConfig(dropout=0.2)

# å¢åŠ  MixUp å¼ºåº¦
training_config = TrainingConfig(mixup_alpha=0.4)

# å‡å°‘æ¨¡å‹å®¹é‡
model_config = ModelConfig(
    num_transformer_layers=2,
    hidden_dims=[64, 32]
)
```

### Q: æ¬ æ‹Ÿåˆ
```python
# å¢åŠ æ¨¡å‹å®¹é‡
model_config = ModelConfig(
    embedding_dim=64,
    num_transformer_layers=4,
    d_ff=256
)

# å…³é—­ MixUp
training_config = TrainingConfig(use_mixup=False)

# å¢åŠ è®­ç»ƒè½®æ•°
training_config = TrainingConfig(epochs=500, patience=30)
```

### Q: GPU å†…å­˜ä¸è¶³
```python
# å‡å° batch size
training_config = TrainingConfig(batch_size=128)

# å‡å°æ¨¡å‹
model_config = ModelConfig(
    embedding_dim=16,
    d_ff=64
)

# å…³é—­ç¼–è¯‘ (ä¼šä½¿ç”¨æ›´å¤šå†…å­˜)
training_config = TrainingConfig(use_compile=False)
```

## ğŸ“ è¾“å‡ºæ–‡ä»¶

è®­ç»ƒå®Œæˆåä¼šç”Ÿæˆä»¥ä¸‹æ–‡ä»¶ï¼š

```
model_checkpoints_v2/
â”œâ”€â”€ best_model_v2.pth      # æœ€ä½³éªŒè¯æŸå¤±æ¨¡å‹
â””â”€â”€ swa_model_v2.pth       # SWA å¹³å‡æ¨¡å‹

model_output/
â”œâ”€â”€ route_mapping.pkl      # è·¯çº¿ç¼–ç æ˜ å°„
â””â”€â”€ preprocessors.pkl      # æ‰€æœ‰é¢„å¤„ç†å™¨

training_history_v2.png    # è®­ç»ƒæ›²çº¿å›¾
predictions_v2.png         # é¢„æµ‹åˆ†æå›¾
```

## ğŸ”— å‚è€ƒæ–‡çŒ®

1. [TabTransformer](https://arxiv.org/abs/2012.06678) - åŸå§‹è®ºæ–‡
2. [FT-Transformer](https://arxiv.org/abs/2106.11959) - æ•°å€¼ç‰¹å¾ Tokenization
3. [Pre-LN Transformer](https://arxiv.org/abs/2002.04745) - Pre-LayerNorm
4. [Lookahead Optimizer](https://arxiv.org/abs/1907.08610) - Lookahead
5. [MixUp](https://arxiv.org/abs/1710.09412) - æ•°æ®å¢å¼º
6. [SWA](https://arxiv.org/abs/1803.05407) - æƒé‡å¹³å‡
